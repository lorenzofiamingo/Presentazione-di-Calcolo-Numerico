\title{Trascritto della presentazione}
\documentclass[a4paper]{article}
\usepackage[margin=0.7in]{geometry}

\usepackage[scaled]{helvet}
\renewcommand\familydefault{\sfdefault} 
\usepackage[T1]{fontenc}


\begin{document}
\maketitle

\section*{Stefano}
\subsection*{Prima slide (metodi di Krylov)}
Buongiorno, oggi spiegheremo i metodi di Krylov: sono metodi utilizzati per risolvere sistemi lineari (anche non simmetrici) in uno dei cosiddetti sottospazi di Krylov.

\subsection*{Bibliografia}
Questi sono i testi su cui abbiamo studiato.

\subsection*{Sommario}
Dopo un’introduzione parleremo di due metodi di Krylov: il metodo del gradiente coniugato, che tratteremo io e Lorenzo, e il metodo GMRES, che sarà trattato da Andrea e Nazariy. 

\subsection*{Introduzione}
I metodi di Krylov risolvono i sistemi lineari Ax=b; come sappiamo A è la matrice dei coefficienti, x è il vettore delle incognite e b il vettore dei termini noti. Questi che spiegheremo sono metodi iterativi, ovvero ad ogni passo viene calcolata la soluzione xk che appartiene al sottospazio di Krylov, che è definito da n colonne in questo modo. 

\subsection*{Introduzione (II)}
Un po’ di definizioni. Chiamiamo x0 il tentativo iniziale di soluzione, il primo vettore che daremo all’algoritmo. La soluzione del sistema la indichiamo con x*, il residuo al passo k sarà rk=b-ax al passo k, e l’errore la soluzione al passo k – la soluzione reale del sistema, che è legato al residuo in questo modo.

\subsection*{Metodi di discesa}
Avendo la matrice a, e ipotizzando che sia simmetrica e definita positiva, definiamo la funzione da Rn a R fi(x), che si chiama forma quadratica. Con le ipotesi che abbiamo imposto sulla matrice a questa funzione è convessa e ha un solo punto di minimo assoluto. La figura mostra un esempio di funzione fi(x).

\subsection*{Metodi di discesa (II)}
Calcoliamo adesso il gradiente di fi(x), e otteniamo che è uguale a ax-b utilizzando le formule a fianco. Sostituendo x* nell’espressione del gradiente otteniamo ax*-b, che è =0 perché è la soluzione esatta del sistema. Il problema di trovare il minimo della funzione fi(x) è quindi equivalente a risolvere il sistema Ax=b.
Partendo da un punto iniziale x0, dobbiamo scendere per passi lungo la superficie fi(x) per raggiungere il punto di minimo: ad ogni iterazione quindi definiamo una direzione di discesa dk, ovvero un vettore che indica la direzione lungo cui scendiamo, definiamo un passo alfak, cioè quanto è lungo il passo di discesa, e troviamo la soluzione al passo successivo seguendo la formula al passo 3.
Adesso spiegheremo il metodo del gradiente e poi più in dettaglio il metodo del gradiente coniugato. Questi due metodi differiscono nella scelta dei metodi di discesa ma hanno in comune la scelta del passo alfak.

\subsection*{Metodo del gradiente}
Scegliamo come direzione di discesa il vettore residuo, come lo avevamo definito all’inizio. Siccome il gradiente di fi(x) è = ax-b, la direzione dk sarà uguale a – il gradiente di fi(x), ovvero la direzione di massima discesa. Il passo di discesa alfak lo scegliamo cosi, e dimostreremo più avanti la formula.
Si aggiorna quindi la soluzione, come avevamo visto, e lo stesso per il vettore residuo.
In figura vediamo le curve di livello di fi(x), e possiamo capire come ha operato il metodo del gradiente coniugato: ad ogni iterazione la direzione di discesa è la direzione del gradiente con verso opposto, e alla fine dopo alcune iterazioni è arrivato alla soluzione.

\subsection*{Metodo del gradiente coniugato – passo di discesa}
Dobbiamo scegliere il passo di discesa, ovvero di quanto scendiamo lungo la direzione dk. L’alfak migliore è quello che minimizza fi valutata in xk+1: si ottiene sostituendo l’espressione di xk+1 nella forma quadratica fi, poi derivando e imponendo la condizione di punto stazionario.

\subsection*{Calcolo del residuo}
Bisogna calcolare il residuo al passo successivo, per poi utilizzarlo nel calcolo della direzione di discesa, secondo questa formula ricavata mediante la definizione di errore. Per matrici particolarmente grandi, ci potrebbero essere errori di troncamento, quindi sarebbe meglio sostituire quest’istruzione con quella citata ogni tanto nell’implementazione.
Ora passo la parola a Lorenzo che continuerà con la scelta della direzione di discesa.


\section*{Lorenzo}
\subsection*{Calcolo del residuo}
Come già visto, il residuo si può ricavare dall’applicazione della matrice A all’errore. Dato che l’errore può essere valutato mediante il passo di discesa nel seguente modo, sostituendo e risolvendo l’equazione, otteniamo che il residuo può essere calcolato utilizzando il residuo e la direzione precedente.
Questa formula non è un’approssimazione, però è soggetta ad errori di troncamento a causa della macchina; soprattutto se la matrice ha  una dimensione elevata. Questo errore di troncamento può essere rimosso applicando, ogni tot iterazioni, la formula originale (che utilizza il vettore dei termini noti, e il vettore della soluzione approssimata a quel passo).

\subsection*{Direzione di discesa}
Abbiamo già visto precedentemente il sottospazio di Krylov K, creato applicando ripetutamente una matrice a un vettore.
Questo sottospazio ha una proprietà importante: infatti se sappiamo che A K è incluso nel sottospazio K del passo successivo e che il residuo del passo successivo è ortogonale  al sottospazio K del passo successivo, allora il residuo del passo successivo è A-ortogonale ad A K.
Di conseguenza il residuo del passo successivo è anche A-ortogonale alle precedenti direzioni di discesa d.
Quindi l’algoritmo di Gram-Schimdt si semplifica e la formula per calcolare la nuova direzione di discesa diviene quella qui mostrata.
È importante notare che non sarà più necessario memorizzare le vecchie direzioni di ricerca.
È proprio questo il punto di forza del complesso coniugato: non dovendo memorizzare le vecchie direzioni, la complessità dello spazio e del tempo si riduce.
Precisamente si riduce da O-grande della dimensione al cubo della matrice, a O-grande lineare degli elementi non nulli della matrice, quindi lineare
È anche interessante notare, che non sono i gradienti a essere coniugati, e che le direzioni non rappresentano la totalità dei gradienti; perciò sarebbe stato più corretto chiamare questo metodo più che del gradiente coniugato delle direzioni coniugate.

\subsection*{Passo di discesa (I)}
Alla luce di queste considerazioni possiamo vedere (anche attraverso la figura due) che a ogni passo il residuo e la direzione puntano allo stesso sottospazio.
Questo sottospazio è parallelo a quello di Krylov da cui sono partiti. Nella figura il sottospazio è piano, perché siamo alla seconda iterazione.
Di conseguenza questi due prodotti scalari si eguagliano, e possiamo riscrivere il numeratore della formula per il calcolo del passo di discesa.
Questa sostituzione, ci permette di migliorare le prestazioni dell’algoritmo.

\subsection*{Passo di discesa (II)}

È complicato effettuare accurate previsioni della convergenza dei metodi iterativi; comunque è possibile ottenere dei limiti utili. Possiamo infatti ottenere la seguente formula dove K di A è il numero di condizionamento della matrice.
Il numero di condizionamento è il rapporto tra il più grande  e il più piccolo degli autovalori della matrice. Questo rappresenta il rapporto tra errore commesso sul risultato di un calcolo e incertezza sui dati in ingresso.
Utilizzando i metodi iterativi raramente ci si trova nella situazione di dover ricercare la soluzione esatta, e molte volte ci si arresta fissando una tolleranza rispetto al residuo.
Per la nostra implementazione lavoreremo fissando una tolleranza sul residuo quadrato in modo da evitare il calcolo di una radice quadrata.

\subsection*{Precondizionamento (I)}
Il precondizionamento è un metodo per migliorare appunto il numero di condizionamento di una matrice: il nostro obbiettivo è quindi quello di raggruppare gli autovalori, ovvero minimizzare la variazione tra l’autovalore maggiore e quello minore.
Intuitivamente lo scopo del precondizionamento è quello di rendere la forma quadratica più sferica.
Per applicare il metodo dobbiamo calcolare il sistema originale A x uguale b mediante la prima equazione che vedete.
Il problema di questa equazione è che M alla meno uno in generale non è simmetrica o definita, ma possiamo comunque trovare una matrice P tale che P per la sua trasposta sia uguale a una matrice M simmetrica e definita positiva.
Il sistema equivalente che ne deriva è questo in fondo, e avremo che l’inversa di P per la matrice A per la traslazione dell’inversa P è simmetrica e definita positiva.

\subsection*{Precondizionamento (II)}
Nella scelta di M stiamo quindi realizzando un compromesso tra la diminuzione del numero di condizionamento, e il calcolo del residuo precondizionato.
Questo trade-off è conveniente al crescere della dimensione di A, quindi conviene spesso calcolare la matrice M; ad esempio attraverso Jacobi, ovvero il precondizionamento diagonale, in cui M presenta sulla diagonale gli stessi elementi della diagonale di A; e il suo effetto è scalare la matrice lungo gli assi delle coordinate.
Più avanzato  è il precondizionamento incompleto di Cholesky. Questo sfrutta la fattorizzazione di Cholesky per fattorizzare A nella forma L per L traslata dove la matrice L è una matrice triangolare bassa. 
Della matrice L per il precondizionamento vengono tenuti solo gli elementi che si trovano nelle stesse posizioni degli elementi non  nulli della matrice A.
Sfortunatamente il metodo di Cholesky non è sempre stabile.

\subsection*{Metodo del gradiente coniugato (senza precondizionamento)}
In questa slide raggruppiamo tutte le formule necessarie per il nostro metodo iterativo.
Con questa visione d’insieme è perciò semplice tradurre il linguaggio matematico in codice Matlab.

\subsection*{Implementazione Matlab (senza precondizionamento)}
Notiamo che abbiamo fissato come condizioni per il ciclo un numero massimo di iterazioni e una tolleranza rispetto alla norma quadrata del residuo.

\subsection*{Metodo del gradiente coniugato (con precondizionamento)}
Analogamente in questa slide abbiamo raggruppato tutte le formule necessarie per il Metodo del gradiente coniugato con precondizionamento.
Di conseguenza il nostro codice Matlab, è di nuovo facilmente ottenibile.

\subsection*{Implementazione Matlab (con precondizionamento)}
Come  detto prima notiamo che a ogni iterazione viene calcolato il residuo precondizionato s.

\subsection*{Considerazioni finali su gradiente e gradiente coniugato}
A fronte di possibili errori di troncamento, comunque evitabili da ciò che abbiamo visto, abbiamo vantaggi importanti.
In definitiva, quello che possiamo concludere, è che attraverso questi metodi (ovvero del gradiente e del gradiente coniugato) possiamo evitare il calcolo dell’inverso della matrice, molto costoso computazionalmente.
Inoltre, diversamente dai metodi che prevedono fattorizzazione; nei casi di matrice sparse manteniamo il vantaggio di allocazione di memoria che queste ci forniscono.
Questi vantaggi ci permettono di allocare meno memoria: sono convenienti specialmente in problemi di grandi dimensioni, e sono utili in ottica di parallelismo su più CPU.

\subsection*{Confronto Gradiente - Gradiente Coniugato}
Confrontando il metodo del gradiente e il metodo del gradiente coniugato possiamo vedere come nel 99\% dei casi converga prima il gradiente coniugato.
Dico 99\%, perché in pochi casi fortunati il gradiente converge alla prima iterazione.
Ad esempio può succedere che se la matrice A ha uguali autovalori e otteniamo la soluzione alla prima iterazione. Si può capire intuitivamente perché se la matrice A ha uguali autovalori le linee di livello della forma quadratica sono circolari; e il punto di minimo si trova sulla direzione del gradiente.
Oppure, se il termine di errore in qualche passo ha la direzione di un autovettore, la soluzione converge al passo successivo.


\section*{Andrea}
\subsection*{Introduzione}

\textbf{[Slide -> Generalized Minimal RESidual]}
Tra i metodi di Krylov vi è il GMRES, ovvero residuo minimo generalizzato. 

\textbf{[Slide -> GMRES]}
Utilizzando il sottospazio Kn si calcola una soluzione approssimata del sistema lineare Ax = b tramite iterazione.
Per calcolare la soluzione esatta del problema sarebbe necessario calcolare $A^-1$ il che risulta eccessivamente dispendioso dal punto di vista computazionale, si procede quindi a calcolare una soluzione approssimata $x_n$ che disterà di un valore b – Axn dalla soluzione esatta; tale valore viene definito residuo. Il GMRES si pone il problema di minimizzare il residuo ottenuto il modo da ottenere la xn che meglio approssima la soluzione del sistema

Se moltiplico Kn per A il residuo equivale a AKnc-b con c un generico vettore appartenente a $C^n$ e c sarà la nostra incognita.
Il valore di c per cui il residuo è minimo è facilmente calcolabile via fattorizzazione QR, però il prodotto AKn diventa numericamente instabile al crescere di n. Per questo motivo occore trovare un nuovo metodo più robusto.
Ci viene in aiuto l’iterazione di Arnoldi .

\textbf{[Slide -> iterazione di Arnoldi(I)]}
Arnoldi consente di semplificare il problema passando da uno di dimensioni m x n a uno di dimensioni (n+1) x n, infatti: definendo come Qn la matrice i cui vettori formano una base ortonormale di Kn
Allora xn può essere scritto come Qn * y e il residuo sarà A Qn y – b

Tramite arnoldi si dimostra che AQn = Qn+1Hn
E otterrò questo la formula a fondo slide

\textbf{[Slide -> iterazione di Arnoldi(II)]}
Successivamente moltiplico entrambi i membri per Qn+1 trasposto, complesso coniugato e noto che Qn+1*b = |b| e1 giungendo in conclusione a Hny - |b|e1. 
Tramite l’approssimazione ai minimi quadrati posso calcolare la y tale per cui la norma del residuo è minima trovando quindi il vettore che meglio approssima la soluzione del sistema lineare Ax = b

\textbf{[Slide -> Generalized Minimal RESidual]}
Il GMRES viene utilizzato anche nel calcolo polinomiale

\textbf{[Slide -> GMRES polinomi]}
Se il polinomio rispetta l’ipotesi pn(0) = 1 (quindi devo semplicemente normalizzare il polinomio rispetto al termine 0) sono in grado di definire una soluzione approssimata xn = qn(A)b avente residuo rn = pn(A)b e dovrò nuovamente minimizzare la norma del residuo. 

\textbf{[Slide -> velocità di convergenza]}
Calcoliamo la velocità di convergenza del GMRES
Innanzitutto vi voglio far notare che la norma di rn qui indicata è quella ottenuta dopo aver risolto il GMRES ed è quindi la più piccola possibile
Procediamo facendo 2 osservazioni:
primo la convergenza è monotona decrescente, quindi al crescere di n ho la garanzia di aver reso più precisa l’approssimazione o nel caso pessimo è rimasta invariata. Infatti la norma di rn è la più piccola possibile nel sottospazio Kn; allora passando al sottospazio a Kn+1 è garantito che rn+1 ha norma <= alla norma di rn
secondo ho la garanzia che per n sufficientemente elevato soddisferò il grado di tolleranza. Questa osservazione è spiegata dal fatto che per n -> infinito posso affermare che la soluzione esatta $x = A^-1 b$ appartiene ad uno dei sottospazi di Krylov considerati quindi avrò un Kn segnato dove la norma minima del residuo è nulla.

La norma del residuo è = alla norma di pn(A)b, ma la norma del prodotto è <= al prodotto delle norme, in conclusione solo pn(A) influenza la velocità di convergneza.


\section*{Nazariy}
\subsection*{Introduzione}
\subsection*{$1^o$frame. Analisi stazionaria efficiente basata sul metodo matrix-free nei sottospazi di Krylov}
Vedremo ora un esempio dell’applicazione del metodo GMRES e di un suo caso particolare.
\subsection*{$2^o$frame. Intro}
L’interesse crescente nei circuiti integrati nei sistemi di comunicazione ha rinnovato anche l’interesse nei vari metodi utilizzati per la loro risoluzione, in particolare per il calcolo della distorsione presente in grandi circuiti analogici. La ricerca di un metodo veloce, ma al tempo stesso accurato, diventa essenziale per una valutazione più rapida del comportamento del circuito. 
Infatti, se volessimo caratterizzare un circuito concreto con la sua distorsione, sarebbero da valutare sia frequenze, sia ampiezze del segnale applicato, quindi bisognerebbe spazzare almeno centinaia di valori per cercare il comportamento del circuito ad ogni frequenza e per ogni ampiezza.
Per raccogliere i dati bisognerebbe simulare il circuito per un certo periodo di tempo, ma in certi casi non è possibile lasciar scorrere abbastanza tempo da ottenere condizioni di stabilità per ogni valore in ingresso, quindi serve un algoritmo rapido ed accurato per caratterizzare un circuito.
Quando si vuole risolvere un tale circuito in condizioni stazionarie, solitamente si fa l’uso del metodo di eliminazione di Gauss, o dei metodi iterativi. Se consideriamo N come il numero di equazioni del circuito, la complessità di calcolo richiesta dal metodo di Gauss è dell’ordine $N^3$, mentre il metodo iterativo richiede $N^2$ di complessità del calcolo. Con questo esempio, mostriamo un caso particolare di GMRES, la cui complessità di calcolo sta nell’ordine di N ed evita la memorizzazione di molte matrici, risparmiando così anche lo spazio sul calcolatore. 
Giusto per dare qualche numero, se si ha un circuito analogico con almeno 400 equazioni, il metodo matrix-free consente di calcolare le soluzioni 10 volte più velocemente dei metodi iterativi.
\subsection*{$3^o$frame. Algoritmo per calcolare la distorsione nei circuiti analogici}
Se volessimo risolvere un circuito con tante equazioni, cercheremmo degli stati stazionari periodici con le seguenti formule.
Infatti, vediamo che i dati si ricavano simulando il circuito e misurando i valori ottenuti in un periodo T. Qui abbiamo vettori di ingressi, di tensioni e correnti ai nodi. Una soluzione dell’equazione (1), che soddisfa anche la (2) è una soluzione periodica stazionaria.
Generalmente, nei metodi di “shooting” la (1) e la (2) si riformulano come la (3), in cui $\phi$ è la funzione di stato dell’equazione (1).
\subsection*{$4^o$frame. Iterazione di Newton (I)}
Di sicuro, non è lo scopo di questa presentazione spiegarvi le iterazioni di Newton, quindi vi mostriamo giusto le formule principali con le quali si fa questo calcolo. Applicando il metodo di Newton alla (3) si ricava l’equazione (4).
Ogni iterazione di Newton richiede la soluzione di un grande sistema lineare, dato dall’equazione 4, che appunto, se fatto con il metodo di Gauss, richiederebbe un costo crescente con il cubo del numero di incognite. Quindi questo metodo risulterebbe intrattabile con un grandissimo numero di equazioni da risolvere.
Con degli opportuni passaggi, descritti appunto più dettagliatamente nel file dac95, allegato nella chat, al paragrafo 2, si arriva alla formula generale risolutiva, data dall’equazione (8).
\subsection*{$5^o$frame. Iterazione di Newton (II)}
La matrice di sensibilità può essere calcolata con la seguente formula, sempre dimostrata nel file pdf. A differenza del metodo di Gauss, il metodo di iterazioni di Newton richiede circa $N^2$ flops di calcolo.
\subsection*{$6^o$frame. Algoritmo (I)}
Vediamo un modo per risolvere il problema con il metodo GMRES. In particolare, vediamo un suo algoritmo semplificato e di facile lettura. 
Qui abbiamo una guess iniziale e la direzione in cui si procede nella ricerca della soluzione finale. Si fa un ciclo iterativo, che ad ogni passo calcola la nuova direzione, la ortogonalizza e cerca $\alpha_k$ tale da minimizzare il residuo. Questo procede avanti fintanto che non soddisfa la tolleranza desiderata.
\subsection*{$7^o$frame. Costo operazioni}
Il costo dominante di questo approccio consiste nel calcolo di N2 componenti di A, tramite l’equazione (11) della slide precedente, ancora prima di cominciare l’iterazione, e di altre N2 operazioni per calcolare $Ap^{k-1}$ ad ogni iterazione.
Vediamo come eseguire questo calcolo con il metodo matrix-free. Essendo un caso particolare di GMRES non richiede un’esplicita rappresentazione di A, ma soltanto il calcolo di $Ap^{k-1}$. Quindi non c’è bisogno di memorizzare la matrice A. (calcolo di $Ap^{k-1}$ non è N quadro?)
\subsection*{$8^o$frame. Formula 12}
Perturbando v(0) in direzione di $p^k$, e integrandolo in un periodo per calcolare $\phi(v_0+\epsilon p^{k-1},0,T)$, si può ricavare $Ap^{k-1}$ tramite l’equazione 12. Anche se la 12 può essere usata direttamente come l’approccio matrix-free per formare i prodotti matrice-vettore richiesti da GMRES per risolvere il problema, sarebbe molto più efficiente, nel senso del costo di computazione e molto più robusto numericamente, salvare $C(v_m)$, eseguire la fattorizzazione LU di $J_f(v_m)$ ad ogni passo e usare la 11 per calcolare $J_{\phi} p^{k-1}$. 
\subsection*{$9^o$frame. Algoritmo (II)}
Questo procedimento è rappresentato nell’algoritmo II.
\subsection*{$10^o$frame. Figura 1}
Vediamo alcuni esempi per capire i vantaggi di questo algoritmo. In figura 1 vengono presentati diversi tipi di circuiti che impiegano diversi componenti, come per esempio xtal che è un filtro a quarzo. Nella seconda colonna viene mostrato il numero di equazioni di ogni circuito. Nella terza colonna viene mostrato il numero di periodi transitori (o iterazioni, non ho ben capito) necessari per raggiungere le condizioni stazionarie. La quarta, quinta e sesta colonna mostra il tempo in secondi, impiegato per eseguire il calcolo delle condizioni stazionarie con il metodo di Gauss, GMRES e algoritmo Matrix-Free. La settima colonna dimostra l’efficacia dell’approccio matrix-free rispetto al metodo di Gauss.
\subsection*{$11^o$frame. Figura 2}
In figura 2 si compara il tempo di ogni iterazione con il costo di calcolo. Il metodo matrix-free dimostra un rapporto pressoché costante, cioè anche all’aumentare del numero delle equazioni, il tempo impiegato per un periodo transitorio (o iterazione, non ho ben capito) rimane invariato.
\subsection*{$12^o$frame. Figura 3}
In figura 3 viene mostrato il grado di convergenza dei risultati dei due metodi per un circuito abbastanza grande. È evidente che l’accuratezza del calcolo rimane sostanzialmente invariata.
Questo approccio può essere applicato anche ad altri problemi. Per esempio, nei circuiti si possono calcolare nello stesso modo anche problemi di bilanciamento armonico e altri problemi che impiegano sia frequenze, sia tempo. 

\end{document}