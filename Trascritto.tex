\title{Trascritto della presentazione}
\documentclass[a4paper]{article}
\usepackage[margin=0.7in]{geometry}

\usepackage[scaled]{helvet}
\renewcommand\familydefault{\sfdefault} 
\usepackage[T1]{fontenc}


\begin{document}
\maketitle

\section*{Stefano}
\subsection*{Prima slide (metodi di Krylov)}
Oggi spiegheremo i metodi di Krylov: sono metodi utilizzati per risolvere sistemi lineari in uno dei cosiddetti sottospazi di Krylov.

\subsection*{Bibliografia}
Questi sono i testi su cui abbiamo studiato.

\subsection*{Sommario}
Dopo un’introduzione parleremo di due metodi di Krylov: il metodo del gradiente coniugato, che tratteremo io e Lorenzo, e il metodo GMRES, che sarà trattato da Andrea. 

\subsection*{Introduzione}
I metodi di Krylov risolvono i sistemi lineari Ax=b; come sappiamo A è la matrice dei coefficienti, che ha dimensione n, x è il vettore delle incognite e b il vettore dei termini noti. Questi che spiegheremo sono metodi iterativi: ad ogni passo viene calcolata la soluzione xk appartenente al sottospazio di Krylov, che aumenta di dimensione ad ogni iterazione. E' definito data una matrice A e un vettore b da k colonne: b, Ab, A quadro b fino a A alla (k-1) b, con k>=1. Quando k sarà uguale a n il sottospazio di Krylov coinciderà con lo spazio Rn, e i vettori colonna che lo definiscono saranno una particolare base di Rn.

Vediamo perchè il sottospazio di Krylov è stato definito in questo modo. Partendo dal metodo di Richardson non stazionario, abbiamo che il vettore residuo rk è uguale a b - A xk, e la soluzione al passo successivo xk+1 è uguale a xk + alfak rk. Sostituendo l'espressione di rk in quest'ultima formula e ponendo la soluzione alla prima iterazione x0 ad esempio = b, possiamo iterare il procedimento calcolando la soluzione x ad ogni passo k.
In questo modo otteniamo che che al k-esimo passo la soluzione xk sarà combinazione lineare dei vettori di base del sottospazio kk.
Quindi, basandosi su questa osservazione, possiamo ridurre il numero di iterazioni necessario per arrivare alla soluzione del sistema attraverso due possibili strategie: il metodo del gradiente coniugato e il metodo gmres.
Inoltre, possiamo implementare questi metodi in versione matrix-free, ovvero ad ogni iterazione basta solo calcolare un prodotto matrice-vettore, e altre operazioni meno computazionalmente costose. Così possiamo allocare meno memoria: questo è vantaggioso quando ci sono problemi di grandi dimensioni. Invece, uno svantaggio dei metodi iterativi è che sono soggetti a errori di troncamento. Nella prossima slide

%\subsection*{Definizioni}
%Diamo alcune definizioni. Chiamiamo x0 il tentativo iniziale di soluzione, cioè il primo vettore che daremo all’algoritmo. La soluzione esatta del sistema la indichiamo con x*, il residuo al passo k sarà il vettore rk=b-ax al passo k, e l’errore la soluzione al passo k – la soluzione esatta del sistema.

\subsection*{Metodo di discesa}
Riformuliamo il problema di ricerca della soluzione del sistema come ricerca di un punto di minimo. Avendo la matrice A, e ipotizzando che sia simmetrica e definita positiva, definiamo la funzione da Rn a R fi(x), che si chiama forma quadratica, come fi(x)= 1/2 x trasposto * A * x - x trasposto * b. Con le ipotesi che abbiamo imposto sulla matrice A questa funzione è convessa, e ha un solo punto di minimo assoluto. La figura mostra un esempio di funzione fi(x).

\subsection*{Metodo di discesa (II)}
Calcoliamo adesso il gradiente di fi(x), e otteniamo che è uguale a ax-b. Sostituendo x* nell’espressione del gradiente otteniamo ax*-b, che è =0 perché x* è la soluzione esatta del sistema. Il problema di trovare il minimo della funzione fi(x) è quindi equivalente a risolvere il sistema Ax=b.
Partendo da un punto iniziale x0, dobbiamo scendere per passi lungo la superficie fi(x) per raggiungere il punto di minimo: ad ogni iterazione quindi definiamo una direzione di discesa dk, ovvero un vettore che indica la direzione lungo cui scendiamo; definiamo un passo alfak, cioè quanto è lungo il passo di discesa, e troviamo la soluzione al passo successivo come xk + alfak per dk.
Il metodo del gradiente coniugato è analogo nell'approccio al metodo del gradiente, ma differisce nella scelta delle direzioni di discesa: si sfrutta la conoscenza delle precedenti, e vengono esplorate le possibili nuove direzioni tramite proiezioni sui sottospazi di Krylov.

\subsection*{Confronto Gradiente Coniugato - Gradiente}
In figura sono rappresentate le curve di livello di fi(x), e le iterazioni necessarie al metodo del gradiente (in verde) e del gradiente coniugato (in rosso) per arrivare a convergenza.
Si nota che il metodo del Gradiente Coniugato esegue molte meno iterazioni rispetto al metodo del Gradiente proprio per questo motivo. 
Ora passo la parola a Lorenzo che continuerà con la scelta della direzione di discesa.

%\subsection*{Metodo del gradiente coniugato – passo di discesa}
%Innanzitutto dobbiamo scegliere il passo di discesa, ovvero di quanto scendiamo lungo la direzione dk. L’alfak migliore è quello che minimizza fi valutata in xk+1, ed è definito come il vettore direzione al passo k trasposto per il vettore residuo rk diviso dk trasposto per A per dk.
%Si ricava questo particolare alfak sostituendo l’espressione di xk+1 nella forma quadratica fi, poi derivando e imponendo la condizione di punto stazionario e ricavando alfak.

%\subsection*{Calcolo del residuo}
%Bisogna calcolare il residuo al passo successivo, per poi utilizzarlo nel calcolo della direzione di discesa. Il vettore residuo al passo k+1 è = al residuo al passo k - alfak per A per il vettore direzione al passo k. La dimostrazione di questa formula si esegue partendo dalla definizione di errore e ricavando rk+1. Per matrici particolarmente grandi, ci potrebbero essere errori di troncamento, quindi sarebbe meglio sostituire ogni tanto nell'algoritmo quest’istruzione con rk = b-A xk.


\section*{Lorenzo}

\subsection*{Direzione di discesa}
Entriamo nel dettaglio del gradiente coniugato, calcolando la direzione di discesa.
Questa, viene scelta in modo che sia A-ortogonale (ovvero coniugata) alle direzioni precedenti. In questo modo possiamo risolvere il problema in un numero di iterazioni, pari al massimo alla dimensione di A.
Per ottenere direzioni A-ortogonali, utilizziamo l’ortogonalizzazione di Gram-Schmidt.
La nuova direzione sarà una combinazione lineare delle precedenti e del residuo.
Calcolarla così è molto dispendioso per la memoria, perché dobbiamo salvarci tutte le nuove direzioni.
Però, i sottostpazi di Krylov hanno una proprietà importante: infatti, se il residuo del passo successivo è ortogonale al sottospazio K del passo successivo, allora il residuo del passo successivo è A-ortogonale a K.
Di conseguenza, il residuo del passo successivo è anche A-ortogonale alle precedenti direzioni di discesa d.
Quindi l’algoritmo di Gram-Schimdt si semplifica e non sarà più necessario memorizzare le vecchie direzioni di ricerca.
La nuova direzione sarà combinazione lineare del residuo e della sola direzione precedente.
È proprio questo il punto di forza del complesso coniugato: non dovendo memorizzare le vecchie direzioni, la complessità dello spazio e del tempo si riduce.
Il risvolto della medaglia è che se in un passo c’è un errore di approssimazione, questo si propaga nelle ortogonalizzazioni successive. Per ovviare a questo si utilizza lo Start And Stop, ovvero il far ripartire l’algoritmo da zero partendo dalla soluzione approssimata che siamo riusciti a calcolare.

\subsection*{Passo di discesa (I)}
Ora che conosciamo in che direzione bisogna muoversi, calcoliamo il passo ottimale da fare. Dato che esploriamo una dimensione alla volta, dobbiamo calcolare il punto per cui in quella direzione la forma quadratica si minimizza.
Per calcolarla sostituiamo nella forma quadratica la formula del passo successivo.
Poi deriviamo, e imponiamo la condizione di stazionarietà.
Da questa formula ricaviamo il nostro passo di discesa.

\subsection*{Passo di discesa (II)}
A ogni passo il residuo e la direzione puntano allo stesso sottospazio, come R2 e D2 in figura.
Questo sottospazio è parallelo a quello di Krylov da cui sono partiti. Nella figura il sottospazio è piano, perché siamo alla seconda iterazione.
Di conseguenza, il prodotto scalare tra direzione e residuo è uguale a quello del residuo per se stesso.
Possiamo riscrivere allora il numeratore della formula per il calcolo del passo di discesa.

\subsection*{Velocità di convergenza}
È complicato effettuare accurate previsioni della convergenza dei metodi iterativi; comunque, è possibile ottenere dei limiti utili per l’errore, che dipendono dal numero di condizionamento K in norma euclidea.
Utilizzando i metodi iterativi, raramente ci si trova nella situazione di dover ricercare la soluzione esatta, e molte volte ci si arresta fissando una tolleranza rispetto al residuo del passo zero.

\subsection*{Residuo}
Ora è necessario trovare una formula più prestante per il calcolo del residuo .
Sappiamo che il residuo è pari alla matrice per l’errore della soluzione approssimata.
Non conosciamo l’errore, ma sappiamo che questo, come la soluzione approssimata, è ottenibile dalla combinazione lineare dell’errore e dalla direzione al passo precedente.
Quindi rimoltiplicando per A troviamo la formula cercata.

\subsection*{Metodo del gradiente coniugato}
Abbiamo ora tutte le conoscenze che ci permettono di definire il nostro algoritmo.
Al passo zero calcoliamo il residuo e lo fissiamo come direzione di partenza.
Entriamo quindi nel ciclo fino alla convergenza:
Calcoliamo l’alpha attraverso il residuo e la direzione.
La soluzione approssimata del passo, la calcoliamo come combinazione lineare della soluzione precedente e della direzione;
Il residuo come combinazione del residuo precedente e di A per la direzione;
A questo punto calcoliamo beta come rapporto delle norme quadrate dei residui del passo in cui siamo e di quello precedente.
Infine la direzione del passo successivo  è la combinazione del residuo e della direzione precedente.
A questo punto traduciamo il linguaggio matematico in codice Matlab.

\subsection*{Implementazione Matlab}
Abbiamo come input la matrice A, il vettore dei termini noti b, una guess iniziale x, un numero massimo di iterazioni kmax e una tolleranza E.
Quindi inizializziamo la variabile delle iterazioni a zero;
Calcoliamo il residuo iniziale, e lo assegnamo alla direzione iniziale ;
Delta è la variabile che memorizza la norma del residuo;
Delta zero, invece, memorizza il residuo al passo zero per poterlo confrontare a ogni passo;
Infatti, come condizioni per il ciclo ritroviamo il numero massimo di iterazioni, e appunto la tolleranza rispetto alla norma quadrata;
Entriamo nel ciclo e assegnamo il prodotto matrice A e direzione a una variabile di convenienza q.
Quindi calcoliamo l’alpha, la soluzione e il residuo seguendo l’algoritmo del gradiente coniugato.
Quindi, dopo aver memorizzato il vecchio residuo in deltaold lo ricalcoliamo.
In questo modo, possiamo calcolarci beta e la direzione per il passo successivo.

\subsection*{Precondizionamento (I)}
Possiamo però spingere ancora più in la le prestazioni del nostro algoritmo attraverso il precondizionamento.
Il precondizionamento è un metodo per migliorare  il numero di condizionamento di una matrice: il nostro obbiettivo è quindi quello di  minimizzare la variazione tra l’autovalore maggiore e quello minore.
Intuitivamente lo scopo del precondizionamento è quello di rendere la forma quadratica più sferica: infatti più è sferica, più l’inverso del suo gradiente punta verso il minimo.
Per fare questo cerchiamo una matrice M, facilmente invertibile, tale che applicando la sua inversa alla matrice A si riduca il suo numero di condizionamento.
Per applicare il metodo dobbiamo calcolare il sistema originale A x uguale b mediante il sistema precondizionato, ottenuto moltiplicando l’inversa di M a entrambi i membri.
Il problema di questa equazione è che M alla meno uno in generale non è simmetrica o definita, nonostante lo siano M ed A.
Possiamo comunque trovare una matrice P tale che P per la sua trasposta sia uguale a una matrice M simmetrica e definita positiva.
Si può dimostrare che l’inversa di M per A ha gli stessi autovalori dell’inversa di P per A per la trasposta inversa di P.
L’inversa di P per la matrice A per la traslazione dell’inversa P è simmetrica e definita positiva. Possiamo quindi risolvere il sistema precondizionato col metodo del gradiente coniugato calcolandoci l’inversa della matrice M come il prodotto della matrice trasposta inversa P per l’inversa di P.

\subsection*{Precondizionamento (II)}
Nella scelta di M stiamo realizzando un compromesso tra la diminuzione del numero di condizionamento, e il calcolo del residuo precondizionato.
Questo trade-off è conveniente al crescere della dimensione di A, quindi conviene spesso calcolare la matrice M;
ad esempio attraverso Jacobi.


\section*{Andrea}
\subsection*{Introduzione}

\subsection*{[Slide -> Generalized Minimal RESidual]}
Un altro metodo di Krylov è il GMRES, ovvero residuo minimo generalizzato. 

\subsection*{[Slide -> GMRES]}
In quanto metodo krylov il GMRES si occupa di approssimare la soluzione del sistema Ax = b a $x_k$.
Il GMRES si pone il problema di minimizzare il residuo di $x_k$ in modo da approssimare al meglio la soluzione del sistema

$X_k$ appartiene a $K_k$, allora potrò scriverlo come combinazione lineare dei vettori del sottospazio di krylov quindi ottengo $x_k$ = c $K_k$ con c un vettore reale di dimensione k. il residuo sarà A $K_k$ c-b.
Il valore di c per cui il residuo è minimo è facilmente calcolabile via fattorizzazione QR.
Si nota subito il difetto di questo metodo: la fattorizzazione QR presenta sempre un errore di troncamento che diventa significativo per k elevati e si genera inutilmente la matrice R. Cerchiamo allora un metodo più robusto per risolvere il sistema.
Ci viene in aiuto l’iterazione di Arnoldi.


\subsection*{[Slide -> iterazione di Arnoldi(I)]}
Tramite Gramm-Schmidt facciamo la fattorizzazione QR delle prime k colonne di A generando così $Q_k$
I vettori di $Q_k$ generano una base ortonormale di $K_k$
Allora posso scrivere $x_k$ come $Q_k$ y dove y appartiene a $R^k$
Sostituendo nella formula del residuo avrò che la norma del residuo è uguale alla norma di A $Q_k$ y – b
Per proseguire nell’iterazione di Arnoldi dobbiamo considerare la matrice di Hessemberg $H_n$.
$H_n$ è una matrice quadrata quasi triangolare superiore aventi dimensioni n x n. 
Una matrice è definita quasi triangolare superiore quando tutti i valori sotto la prima sottodiagonale sono pari a 0.
Consideriamo la sezione superiore sinistra di $H_n$ che chiamo $H_k$ ottenuta considerando le prime k+1 righe e k colonne di $H_n$. $H_k$ avrà dimensioni k+1 x k.


\subsection*{[Slide -> iterazione di Arnoldi(II)]}
Per comprendere meglio come si ricavano $Q_k$ e $H_k$ scriviamo l’algoritmo che le genera:
la prima colonna di q è = a b/norm(b)
procedo ora con l’terazione per k da 1 a $num_iter$
memorizzo in v il prodotto A per la k esima colonna di q
poi for j = 1:k genero le righe della k esima colonna di h: $h(j,k)=q_k’ a q_k$
calcolo h(k+1,k) = norm(v) e la k+1 esima colonna di q come v/h(k+1,k).
Soffermiamoci sull’ultima formula

\subsection*{[Slide -> iterazione di Arnoldi(III)]}

Sostituiamo v con il suo valore: A $q_k$ e otteniamo che la k+1 esima colonna di q può essere calcolata come$ q_k+1 = A q_k / h_k$
Moltiplicando entrambi i termini per $h_k$ A $q_k$ = $q_k+1 h_k $
Questa formula vale anche in forma matriciale
La norma da minimizzare sarà norma di$ Q_k+1 H_k y - b$


\subsection*{[Slide -> iterazione di Arnoldi(IV)]}

Successivamente moltiplico entrambi i membri per Qk+1 trasposto e noto che Qk+1*b = |b| e1 giungendo in conclusione il problema sarà di $H_k y - |b|e1 = minimo$. 
Tramite l’approssimazione ai minimi quadrati posso calcolare la y tale per cui la norma del residuo è minima trovando quindi il vettore che meglio approssima la soluzione del sistema lineare Ax = b.
L’utilizzo dell’ iterazione di arnoldi è molto vantaggioso infatti:
Con la fattorizzazione QR il problema:
1)ha dimensioni m x m
2) non è ridotto ai minimi quadrati
3)gli errori di troncamento della fattorizzazione QR posso essere non trascurabili
4)la matrice R è generata inutilmente
Con Arnoldi il problema:
1)ha dimensione (k + 1) x k con k < m
2)è ridotto ai minimi quadrati
3)limitando il numero di colonne di A su cui faccio la fattorizzazione QR riduco gli errori di troncamento
4)generiamo inutilmente la matrice $R_k$ che però ha dimensioni più piccole rispetto alla R prima generata

\subsection*{[Slide -> Metodi di Krylov nell'elettronica(I)]}
Vediamo uno dei possibili settori in cui vengono utilizzati i metodi di krylov. Non a caso scegliamo un esempio a noi caro, i circuiti analogici. 
Esistono circuiti aventi centinaia di componenti ognuno dei quali è descritto da molteplici equazioni.
Consideriamo il generico circuito analogico descritto da un totale N equazioni che rappresento col sistema Ax = b.
Come primo tentativo usiamo l’eliminazione di Gauss, così facendo scriveremmo un algoritmo richiedente un numero di flops che cresce come $N^3$. Proviamo allora ad utilizzare dei metodi iterativi:
molti metodi iterativi tra cui il GMRES riescono a ridurre il numero di flops a circa $N^2$, ma esiste un metodo ancora più vantaggioso che ci porta a dover compiere un numero di operazioni che cresce come N; questo metodo è il GMRES matrix free.
Che differenza si ha tra il GMRES standard e quello matrix free? Perché il metodo matrix free ci permette di risparmiare così tanto?
Per trovare la risposta a queste domande consideriamo il seguente circuito

\subsection*{[Slide -> Metodi di Krylov nell'elettronica(II)]}
Un circuito così piccolo forma una matrice sparsa, allora sarete tutti d’accordo se dovessi dire che circuiti con centinaia di nodi formeranno matrici con ancora più valori nulli dato che è praticamente impossibile collegarli tutti assieme. Il GMRES Matrix Free ci consente di memorizzare solo i valori non nulli di A assieme al loro indirizzo nella matrice: quindi in memoria avremo una struttura del tipo: valore, numero di riga, numero colonna.
 Per esempio il valore -G1 che si trova nella riga 1 seconda colonna di A lo memorizzeremmo come -G1, 1, 2.
Così facendo si risparmia molto spazio e i calcoli in cui si utilizza la matrice A richiederanno molti meno flops


\subsection*{Slide -> Immagine}
Nell’immagine vedete i vari metodi messi a confronto. Le prime due colonne dicono rispettivamente il nome del componente e il numero di equazioni che lo descrivono. Le 3 colonne successive rappresentano il tempo in secondi impiegato per risolvere il circuito rispettivamente con eliminazione di gauss, GMRES e GMRES matrix free infine l’ultima colonna mette a confronto l’eliminazione di Gauss con il gmres matrix free

\end{document}
